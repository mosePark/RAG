# %% ì±— ë””ë²„ê¹…

# %% ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import ollama
from ollama import ChatResponse, Message

# %% ì§ˆë¬¸ ë° ëª¨ë¸ ì„¤ì •
question = input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ")
model_name = "qwen3:4b"
messages = [
    Message(role="system", content="You are a helpful assistant."),
    Message(role="user", content=question)
]

# I%% ëª¨ë¸ì— ë‹¨ì¼ ì‘ë‹µ ìš”ì²­
response: ChatResponse = ollama.chat(model=model_name, messages=messages, stream=False)

# %% ì‘ë‹µ ë‚´ìš© í™•ì¸ ë° ì¶œë ¥
if hasattr(response, 'message') and response.message:
    answer = response.message.content
    print("Answer:", answer)
else:
    # response.error í•„ë“œ ë˜ëŠ” ê¸°íƒ€ ì†ì„± í™•ì¸
    error_msg = getattr(response, 'error', None)
    print("[ERROR]", error_msg or "ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

# %% ë””ë²„ê¹…ìš© ì „ì²´ ê²°ê³¼ ì¶œë ¥
print("[DEBUG] ollama.chat ë°˜í™˜ê°’:", response)

# %% ì—˜ë¼ìŠ¤í‹± ì„œì¹˜ ë””ë²„ê¹…

# %%

from src.embeddings import EmbeddingStore
from src.retrieval import retrieve_documents

query = "ê³µë‹¨ì˜ ê´€ì¥ ì—…ë¬´ë¥¼ ë‚˜ì—´í•˜ë¼"
store = EmbeddingStore()
q_emb = store.embedder.embed_query(query)

docs = retrieve_documents(query, store, top_k=3)
docs

# %% ì‹œê°„ ì¬ë³´ê¸°

import time
from src.embeddings import EmbeddingStore
from src.retrieval import retrieve_documents
from src.chat import generate_answer, msgManager

query = "êµ­ë¯¼ê±´ê°•ë³´í—˜ê³µë‹¨ì˜ ì—…ë¬´ì— ëŒ€í•´ ì •ë¦¬í•´ì¤˜."
store = EmbeddingStore()

# 1. ë¬¸ì„œ ê²€ìƒ‰ ì‹œê°„ ì¸¡ì •
start = time.time()
docs = retrieve_documents(query, store, top_k=3)
print(f"\nâ±ï¸ ë¬¸ì„œ ê²€ìƒ‰ ì‹œê°„: {time.time() - start:.3f}ì´ˆ")

# 2. í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹œê°„ ì¸¡ì •
start = time.time()
msgManager.append_msg(query)  # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
msg = msgManager.generate_prompt(docs)
print(f"â±ï¸ í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹œê°„: {time.time() - start:.3f}ì´ˆ")

# 3. ëª¨ë¸ ì‘ë‹µ ì‹œê°„ ì¸¡ì •
start = time.time()
full_answer = ""
print("RAG ì‘ë‹µ (stream): ", end="", flush=True)
import ollama
model = "qwen3:4b"  # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ì§ì ‘ ì„¤ì •

for response in ollama.chat(model=model, messages=msg, stream=True):
    if hasattr(response, "message") and response.message:
        content = response.message.content or ""
    else:
        content = response.get("message", {}).get("content", "")
    print(content, end="", flush=True)
    full_answer += content

print(f"\nâ±ï¸ ëª¨ë¸ ì‘ë‹µ ì‹œê°„: {time.time() - start:.3f}ì´ˆ")

# 4. ê²°ê³¼ ìš”ì•½
print("\nâœ… ì „ì²´ ìš”ì•½ ì™„ë£Œ")

# %% ë³‘ëª© ì§€ì  ì‹ë³„

import tiktoken

def count_tokens_from_messages(messages, model="gpt-3.5-turbo"):
    """
    messages: List of dicts like [{"role": "system", "content": "..."}, ...]
    """
    encoding = tiktoken.encoding_for_model(model)
    total_tokens = 0
    for message in messages:
        total_tokens += 4  # role & formatting tokens (OpenAI ê¸°ì¤€)
        for key, value in message.items():
            total_tokens += len(encoding.encode(value))
    total_tokens += 2  # reply primer
    return total_tokens

#%%

# í”„ë¡¬í”„íŠ¸ ìƒì„±
msgManager.append_msg(query)  # ì‚¬ìš©ì ì§ˆë¬¸ ì¶”ê°€
prompt = msgManager.generate_prompt(docs)

# í† í° ìˆ˜ ì¸¡ì •
tokens = count_tokens_from_messages(prompt)
print(f"ğŸ“ ì´ í”„ë¡¬í”„íŠ¸ í† í° ìˆ˜: {tokens} tokens")
# %%

print("ğŸ“„ ë¬¸ì„œ ê¸¸ì´:")
for i, doc in enumerate(docs):
    print(f"[{i+1}] {len(doc)}ì / {len(tiktoken.encoding_for_model('gpt-3.5-turbo').encode(doc))} tokens")
